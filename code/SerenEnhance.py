# -*- coding: utf-8 -*-
"""FGS_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JoZPSf1ia-wV_gcHpbP4C_wurdJhVaem
"""

import tensorflow as tf
print("TensorFlow version:", tf.__version__)
print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import Model

import numpy as np
import tensorflow as tf
import os


class DataBuilder(tf.keras.utils.Sequence):
    def __init__(self, file_dir, t_path, r_path, u_path, batch_size, n_reviews, user_threshold, test=False):
        # other parameters
        self.file_dir = file_dir
        self.t_path = t_path
        self.r_path = r_path
        self.u_path = u_path
        self.user_list = np.array(os.listdir(file_dir))
        self.user_list_length = len(self.user_list)
        self.indexes = np.arange(self.user_list_length)
        self.batch_size = batch_size
        self.n_reviews = n_reviews
        self.user_threshold = user_threshold
        self.test = test

    def __len__(self):
        # Denotes the number of batches per epoch
        if self.test:
          return int((len(self.user_list) * (1-self.user_threshold)) // self.batch_size + 1)
        else:
          return int((len(self.user_list) * self.user_threshold) // self.batch_size + 1)


    def __getitem__(self, index):
        # Generate indexes of the batch
        last_index = (index + 1) * self.batch_size
        if self.test:
          if last_index < self.user_list_length*(1-self.user_threshold):
              indexes = self.indexes[(-1 * last_index)-1: (-1 * index * self.batch_size)-1]
          else:
              indexes = self.indexes[(-1 * int(self.user_list_length * (1-self.user_threshold)))-1 : (-1 * index * self.batch_size)-1]
        else:
          if last_index < (self.user_list_length*self.user_threshold):
              indexes = self.indexes[index * self.batch_size:last_index]
          else:
              indexes = self.indexes[index * self.batch_size:]
        # if self.test==False:
        #   if last_index < self.user_list_length*(1-self.user_threshold):
        #       indexes = self.indexes[(-1 * last_index)-1: (-1 * index * self.batch_size)-1]
        #   else:
        #       indexes = self.indexes[(-1 * int(self.user_list_length * (1-self.user_threshold)))-1 : (-1 * index * self.batch_size)-1]
        # else:
        #   if last_index < (self.user_list_length*self.user_threshold):
        #       indexes = self.indexes[index * self.batch_size:last_index]
        #   else:
        #       indexes = self.indexes[index * self.batch_size:]
        # Generate data
        return self.__dataGeneration(self.user_list[indexes])


    def __dataGeneration(self, user_list_batch):
        # read user csv
        users_arr = tf.zeros((0, self.n_reviews, 128), dtype=tf.float32)
        ts_arr = tf.zeros((0, 100, 128), dtype=tf.float32)
        rs_arr = tf.zeros((0, 100, 128), dtype=tf.float32)
        us_arr = tf.zeros((0, 100, 128), dtype=tf.float32)
        for user in user_list_batch:
            user_arr = np.genfromtxt(os.path.join(self.file_dir, user), delimiter=',', dtype=np.float32)  # array(r, 128)
            user_arr = np.expand_dims(self.sliceOrPad(user_arr, self.n_reviews), axis=0)  # array(1, n_reviews, 128)
            users_arr = np.concatenate([users_arr, user_arr], axis=0)


            t_arr = np.genfromtxt(os.path.join(self.t_path, user), delimiter=',', dtype=np.float32)
            t_arr = np.expand_dims(self.sliceOrPad(t_arr[1:], 100), axis=0)

            t_pos = tf.zeros((0, 128), dtype=tf.float32)
            t_neg = tf.zeros((0, 128), dtype=tf.float32)
            for i in range(len(t_arr)):
              for j in range(len(t_arr[i])):
                if j%2!=0:
                  t_neg = np.concatenate([t_neg, t_arr[i][j].reshape((1,-1))], axis=0)
                else:
                  t_pos = np.concatenate([t_pos, t_arr[i][j].reshape((1,-1))], axis=0)
              t_arr_pn = np.concatenate([t_pos, t_neg], axis=1)
              ts_arr = np.concatenate([ts_arr, t_arr_pn.reshape((1,-1,128))], axis=0)

            r_arr = np.genfromtxt(os.path.join(self.r_path, user), delimiter=',', dtype=np.float32)
            r_arr = np.expand_dims(self.sliceOrPad(r_arr[1:], 100), axis=0)
            r_pos = tf.zeros((0, 128), dtype=tf.float32)
            r_neg = tf.zeros((0, 128), dtype=tf.float32)
            for i in range(len(r_arr)):
              for j in range(len(r_arr[i])):
                if j%2!=0:
                  r_neg = np.concatenate([r_neg, r_arr[i][j].reshape((1,-1))], axis=0)
                else:
                  r_pos = np.concatenate([r_pos, r_arr[i][j].reshape((1,-1))], axis=0)
              r_arr_pn = np.concatenate([r_pos, r_neg], axis=1)
              rs_arr = np.concatenate([rs_arr, r_arr_pn.reshape((1,-1,128))], axis=0)

            u_arr = np.genfromtxt(os.path.join(self.u_path, user), delimiter=',', dtype=np.float32)[:,1:129]
            u_arr = np.expand_dims(self.sliceOrPad(u_arr[1:], 100), axis=0)
            u_pos = tf.zeros((0, 128), dtype=tf.float32)
            u_neg = tf.zeros((0, 128), dtype=tf.float32)
            for i in range(len(u_arr)):
              for j in range(len(u_arr[i])):
                if j>49:
                  u_neg = np.concatenate([u_neg, u_arr[i][j].reshape((1,-1))], axis=0)
                else:
                  u_pos = np.concatenate([u_pos, u_arr[i][j].reshape((1,-1))], axis=0)
              u_arr_pn = np.concatenate([u_pos, u_neg], axis=1)
              us_arr = np.concatenate([us_arr, u_arr_pn.reshape((1,-1,128))], axis=0)

            # ts_arr = np.concatenate([ts_arr, t_arr], axis=0)

        return users_arr, ts_arr, rs_arr, us_arr  # array(batch_size, n_reviews, 128)

    @staticmethod
    def sliceOrPad(arr, threshold):
        row_num = arr.shape[0]
        if row_num < threshold:
            return np.pad(arr, ((threshold - row_num, 0), (0, 0)), 'constant')
        elif row_num > threshold:
            return arr[(-threshold-1):-1, :]
        else:
            return arr

user_data_path = 'D:/WWW/data/user_pre_encoded_sorted'
train_path = 'D:/WWW/data/user_pre'

rel_path = 'D:/WWW/data/user_rel_train'
unp_path = 'D:/WWW/data/user_unexpectedness_samples'
sre_path = 'D:/WWW/data/user_pre_candidate'
test_path = 'D:/WWW/data/user_pre_test'
# sre_path = 'D:/WWW/data/user_rel_train'
# test_path = 'D:/WWW/data/user_rel_test'

user_batch_size = 32
user_n_reviews = 20  # Only consider the last n reviews of a user
embed_dim = 128  # Embedding size for each token
n_heads = 2  # Number of attention heads
ff_dim = 32  # Hidden layer size in feed forward network inside transformer
user_threshold = 0.8 # split dataset

# from databuilder import DataBuilder
data_all = DataBuilder(user_data_path, sre_path, rel_path, unp_path, user_batch_size, user_n_reviews, user_threshold)
data_train = data_all[0][0]
sre_train = data_all[0][1]
rel_train = data_all[0][2]
unp_train = data_all[0][3]
for i in range(1, len(data_all)-1):
  data_train = np.concatenate([data_train, data_all[i][0]], axis=0)
  sre_train = np.concatenate([sre_train, data_all[i][1]], axis=0)
  rel_train = np.concatenate([rel_train, data_all[i][2]], axis=0)
  unp_train = np.concatenate([unp_train, data_all[i][3]], axis=0)


data_all = DataBuilder(user_data_path, test_path, rel_path, unp_path, user_batch_size, user_n_reviews, user_threshold, test=True)
data_test = data_all[0][0]
sre_test = data_all[0][1]
rel_test = data_all[0][2]
unp_test = data_all[0][3]
for i in range(1, len(data_all)):
  data_test = np.concatenate([data_test, data_all[i][0]], axis=0)
  sre_test = np.concatenate([sre_test, data_all[i][1]], axis=0)
  rel_test = np.concatenate([rel_test, data_all[i][2]], axis=0)
  unp_test = np.concatenate([unp_test, data_all[i][3]], axis=0)

"""transformer"""

class TransformerBlock(tf.keras.layers.Layer):

    def __init__(self, embed_dim, n_heads, ff_dim, rate=0.1):
        super().__init__()
        # Multi-Head Attention
        self.att = tf.keras.layers.MultiHeadAttention(num_heads=n_heads, key_dim=embed_dim)
        # Feed Forward
        self.ffn = tf.keras.Sequential(
            [tf.keras.layers.Dense(ff_dim, activation="relu"), tf.keras.layers.Dense(embed_dim),]
        )
        # Normalization
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        # Dropout
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def __call__(self, inputs, training=True):
        # Part 1. Multi-Head Attention + Normalization + Residual
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        # Part 2. Feed Forward + Normalization + Residual
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

class TokenAndPositionEmbedding(tf.keras.layers.Layer):

    def __init__(self, n_reviews, embed_dim):
        super().__init__()
        # Compute the positional encodings
        self.pe = tf.Variable(tf.zeros((n_reviews, embed_dim), dtype=tf.float32), dtype=tf.float32)
        position = tf.expand_dims(tf.range(0, n_reviews, 1, dtype=tf.float32), axis=1)
        div_term = tf.math.exp(tf.range(0, embed_dim, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / embed_dim))
        term = tf.math.multiply(position, div_term)
        self.pe = self.pe[:, 0::2].assign(tf.math.sin(term))
        self.pe = self.pe[:, 1::2].assign(tf.math.cos(term))
        self.pe = tf.expand_dims(self.pe, axis=0)

    def __call__(self, x):
        return x + tf.broadcast_to(self.pe, tf.shape(x))

"""main"""

from sklearn.metrics.pairwise import pairwise_kernels
from tensorflow.keras import regularizers

class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    embed_dim = 128  # Embedding size for position
    n_heads = 2  # Number of attention heads
    ff_dim = 32  # Hidden layer size in feed forward network inside transformer
    self.num_blocks = 3 # Number of stacking Trm blocks
    n_reviews = 20

    dim = 128 # dimension for clusters/items/preferences
    batch_size = 32


    self.d1 = layers.Dense(dim, activation='relu')
    self.d2 = layers.Dense(dim,activation='relu')
    self.d3 = layers.Dense(dim,activation='relu')

    self.embedding_layer = TokenAndPositionEmbedding(n_reviews, embed_dim)
    self.transformer_block1 = TransformerBlock(embed_dim, n_heads, ff_dim)
    self.transformer_block2 = TransformerBlock(embed_dim, n_heads, ff_dim)
    self.transformer_block3 = TransformerBlock(embed_dim, n_heads, ff_dim)

    self.a = 0.4
    self.dis = 0


  def call(self, x, y_seren, y_rel, y_unp):
    # position embedding
    x = self.embedding_layer(x)

    # serendipity
    for i in range(self.num_blocks):
      x_seren = self.transformer_block1(x)
    x_seren = layers.GlobalAveragePooling1D()(x_seren)
    x_seren = self.d1(x_seren)
    x_seren = layers.Dropout(0.2)(x_seren)
    x_seren = tf.reshape(x_seren,[-1,1,x_seren.shape[-1]])

    r_seren = tf.matmul(x_seren, y_seren, transpose_b=True)
    r_seren = tf.reshape(r_seren,[-1,r_seren.shape[-1]])
    r_seren = tf.keras.activations.sigmoid(r_seren)
    r_seren_pos = r_seren[:,:50]
    r_seren_neg = r_seren[:,50:]
    pair_seren_r = r_seren_pos - r_seren_neg

    # Relevance
    for i in range(self.num_blocks):
      x_rel = self.transformer_block2(x)
    x_rel = layers.GlobalAveragePooling1D()(x_rel)
    x_rel = self.d2(x_rel)
    x_rel = layers.Dropout(0.2)(x_rel)
    x_rel = tf.reshape(x_rel,[-1,1,x_rel.shape[-1]])

    r_rel = tf.matmul(x_rel, y_rel, transpose_b=True)
    r_rel = tf.reshape(r_rel,[-1,r_rel.shape[-1]])
    r_rel = tf.keras.activations.sigmoid(r_rel)
    r_rel_pos = r_rel[:,:50]
    r_rel_neg = r_rel[:,50:]
    pair_rel_r = r_rel_pos - r_rel_neg

    # Unexpectedness
    for i in range(self.num_blocks):
      x_unp = self.transformer_block3(x)
    x_unp = layers.GlobalAveragePooling1D()(x_unp)
    x_unp = self.d2(x_unp)
    x_unp = layers.Dropout(0.2)(x_unp)
    x_unp = tf.reshape(x_unp,[-1,1,x_unp.shape[-1]])

    r_unp = tf.matmul(x_unp, y_unp, transpose_b=True)
    r_unp = tf.reshape(r_unp,[-1,r_unp.shape[-1]])
    r_unp = tf.keras.activations.sigmoid(r_unp)
    r_unp_pos = r_unp[:,:50]
    r_unp_neg = r_unp[:,50:]
    pair_unp_r = r_unp_pos - r_unp_neg

    # merging layer

    r = x_seren + self.a * x_rel + (1-self.a) * x_unp
    r = tf.matmul(r, y_seren, transpose_b=True)
    r = tf.nn.softmax(tf.reshape(r,[-1,r.shape[-1]]))

    return r, pair_seren_r, pair_rel_r, pair_unp_r

# Create an instance of the model
model = MyModel()

"""optimizer"""

loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)
# loss_object = tf.keras.losses.MeanSquaredError()

optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)

"""metrics"""

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy_1 = tf.keras.metrics.TopKCategoricalAccuracy(k=1, name='train_accuracy')
train_accuracy_5 = tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='train_accuracy')
train_accuracy_10 = tf.keras.metrics.TopKCategoricalAccuracy(k=10, name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy_1 = tf.keras.metrics.TopKCategoricalAccuracy(k=1, name='test_accuracy')
test_accuracy_5 = tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='test_accuracy')
test_accuracy_10 = tf.keras.metrics.TopKCategoricalAccuracy(k=10, name='test_accuracy')

"""data prepare"""

test_labels = np.zeros((sre_test.shape[0],sre_test.shape[1]))
test_labels[:,0]=1
test_labels = np.reshape(test_labels,(sre_test.shape[0],sre_test.shape[1],1))
test_y = np.concatenate((sre_test,test_labels),axis=2)
test_y = np.array(test_y, dtype=np.float32)


rel_test_labels = np.zeros((rel_test.shape[0],rel_test.shape[1]))
rel_test_labels[:,0]=1
rel_test_labels = np.reshape(rel_test_labels,(rel_test.shape[0],rel_test.shape[1],1))
rel_test_y = np.concatenate((rel_test,rel_test_labels),axis=2)
rel_test_y = np.array(rel_test_y, dtype=np.float32)

unp_test_labels = np.zeros((unp_test.shape[0],unp_test.shape[1]))
unp_test_labels[:,0]=1
unp_test_labels = np.reshape(unp_test_labels,(unp_test.shape[0],unp_test.shape[1],1))
unp_test_y = np.concatenate((unp_test,unp_test_labels),axis=2)
unp_test_y = np.array(unp_test_y, dtype=np.float32)

# training labels
train_labels = np.zeros((sre_train.shape[0],sre_train.shape[1]))
train_labels[:,:50] = 1

rel_train_labels = np.zeros((rel_train.shape[0],rel_train.shape[1]))
rel_train_labels[:,:50] = 1

unp_train_labels = np.zeros((unp_train.shape[0],unp_train.shape[1]))
unp_train_labels[:,:50] = 1

train_ds = tf.data.Dataset.from_tensor_slices((data_train, sre_train, train_labels, rel_train, rel_train_labels, unp_train, unp_train_labels)).batch(32)

test_ds = tf.data.Dataset.from_tensor_slices((data_test, test_y[:,:,:-1], test_y[:,:,-1], rel_test_y[:,:,:-1], rel_test_y[:,:,-1], unp_test_y[:,:,:-1], unp_test_y[:,:,-1])).batch(32)

"""training step"""

def train_step(data_train, sre_train, sre_labels, rel_train, rel_labels, unp_train, unp_labels):
  with tf.GradientTape() as tape:
    # training=True is only needed if there are layers with different
    # behavior during training versus inference (e.g. Dropout).
    predictions, pair_sre, pair_rel, pair_unp = model(data_train, sre_train, rel_train, unp_train, training=True)

    loss_sre = loss_object(sre_labels[:,:50], pair_sre)
    loss_rel = loss_object(rel_labels[:,:50], pair_rel)
    loss_unp = loss_object(unp_labels[:,:50], pair_unp)

    loss = loss_sre + loss_rel + loss_unp
    # print(loss)

  gradients = tape.gradient(loss, model.trainable_variables)
  # print(gradients)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))

  train_loss(loss)
  train_accuracy_1(sre_labels, predictions)
  train_accuracy_5(sre_labels, predictions)
  train_accuracy_10(sre_labels, predictions)

"""test step"""

from sklearn.metrics import ndcg_score

def test_step(data_test, sre_test, sre_labels, rel_test, rel_labels, unp_test, unp_labels):
  # training=False is only needed if there are layers with different
  # behavior during training versus inference (e.g. Dropout).
  predictions, pair_sre, pair_rel, pair_unp = model(data_test, sre_test, rel_test, unp_test, training=True)
  t_loss = loss_object(sre_labels, predictions)

  test_loss(t_loss)
  test_accuracy_1(sre_labels, predictions)
  test_accuracy_5(sre_labels, predictions)
  test_accuracy_10(sre_labels, predictions)

  ndcg_1 = ndcg_score(sre_labels, predictions, k=1)
  ndcg_5 = ndcg_score(sre_labels, predictions, k=5)
  ndcg_10 = ndcg_score(sre_labels, predictions, k=10)

  return ndcg_1, ndcg_5, ndcg_10

EPOCHS = 100

for epoch in range(EPOCHS):
  # Reset the metrics at the start of the next epoch
  train_loss.reset_states()
  test_loss.reset_states()
  test_accuracy_1.reset_states()
  test_accuracy_5.reset_states()
  test_accuracy_10.reset_states()

  for data, sre_train, sre_label, rel_train, rel_label, unp_train, unp_label in train_ds:
    train_step(data, sre_train, sre_label, rel_train, rel_label, unp_train, unp_label)

  for test_data, sre_test, sre_label, rel_test, rel_label, unp_test, unp_label in test_ds:
    ndcg_1, ndcg_5,ndcg_10 = test_step(test_data, sre_test, sre_label, rel_test, rel_label, unp_test, unp_label)


  print(
    f'Epoch {epoch + 1}, '
    f'Loss: {train_loss.result()}, '
    f'Test Loss: {test_loss.result()}, '
    f'Test HR@1: {test_accuracy_1.result() * 100},'
    f'Test HR@5: {test_accuracy_5.result() * 100},'
    f'Test HR@10: {test_accuracy_10.result() * 100},'
    # 'Test NDCG@1: ', ndcg_1,
    'Test NDCG@5: ', ndcg_5,
    'Test NDCG@10: ', ndcg_10,
  )

EPOCHS = 100

for epoch in range(EPOCHS):
  # Reset the metrics at the start of the next epoch
  train_loss.reset_states()
  test_loss.reset_states()
  test_accuracy_1.reset_states()
  test_accuracy_5.reset_states()
  test_accuracy_10.reset_states()

  for data, sre_train, sre_label, rel_train, rel_label, unp_train, unp_label in train_ds:
    train_step(data, sre_train, sre_label, rel_train, rel_label, unp_train, unp_label)

  for test_data, sre_test, sre_label, rel_test, rel_label, unp_test, unp_label in test_ds:
    ndcg_1, ndcg_5,ndcg_10 = test_step(test_data, sre_test, sre_label, rel_test, rel_label, unp_test, unp_label)


  print(
    f'Epoch {epoch + 1}, '
    f'Loss: {train_loss.result()}, '
    f'Test Loss: {test_loss.result()}, '
    f'Test HR@1: {test_accuracy_1.result() * 100},'
    f'Test HR@5: {test_accuracy_5.result() * 100},'
    f'Test HR@10: {test_accuracy_10.result() * 100},'
    # 'Test NDCG@1: ', ndcg_1,
    'Test NDCG@5: ', ndcg_5,
    'Test NDCG@10: ', ndcg_10,
  )

EPOCHS = 100

for epoch in range(EPOCHS):
  # Reset the metrics at the start of the next epoch
  train_loss.reset_states()
  test_loss.reset_states()
  test_accuracy_1.reset_states()
  test_accuracy_5.reset_states()
  test_accuracy_10.reset_states()

  for data, sre_train, sre_label, rel_train, rel_label, unp_train, unp_label in train_ds:
    train_step(data, sre_train, sre_label, rel_train, rel_label, unp_train, unp_label)

  for test_data, sre_test, sre_label, rel_test, rel_label, unp_test, unp_label in test_ds:
    ndcg_1, ndcg_5,ndcg_10 = test_step(test_data, sre_test, sre_label, rel_test, rel_label, unp_test, unp_label)


  print(
    f'Epoch {epoch + 1}, '
    f'Loss: {train_loss.result()}, '
    f'Test Loss: {test_loss.result()}, '
    f'Test HR@1: {test_accuracy_1.result() * 100},'
    f'Test HR@5: {test_accuracy_5.result() * 100},'
    f'Test HR@10: {test_accuracy_10.result() * 100},'
    # 'Test NDCG@1: ', ndcg_1,
    'Test NDCG@5: ', ndcg_5,
    'Test NDCG@10: ', ndcg_10,
  )

EPOCHS = 100

for epoch in range(EPOCHS):
  # Reset the metrics at the start of the next epoch
  train_loss.reset_states()
  test_loss.reset_states()
  test_accuracy_1.reset_states()
  test_accuracy_5.reset_states()
  test_accuracy_10.reset_states()

  for data, sre_train, sre_label, rel_train, rel_label, unp_train, unp_label in train_ds:
    train_step(data, sre_train, sre_label, rel_train, rel_label, unp_train, unp_label)

  for test_data, sre_test, sre_label, rel_test, rel_label, unp_test, unp_label in test_ds:
    ndcg_1, ndcg_5,ndcg_10 = test_step(test_data, sre_test, sre_label, rel_test, rel_label, unp_test, unp_label)


  print(
    f'Epoch {epoch + 1}, '
    f'Loss: {train_loss.result()}, '
    f'Test Loss: {test_loss.result()}, '
    f'Test HR@1: {test_accuracy_1.result() * 100},'
    f'Test HR@5: {test_accuracy_5.result() * 100},'
    f'Test HR@10: {test_accuracy_10.result() * 100},'
    # 'Test NDCG@1: ', ndcg_1,
    'Test NDCG@5: ', ndcg_5,
    'Test NDCG@10: ', ndcg_10,
  )

for test_data, sre_test, sre_label, rel_test, rel_label, unp_test, unp_label in test_ds:
    ndcg_1, ndcg_5, ndcg_10 = test_step(test_data, sre_test, sre_label, rel_test, rel_label, unp_test, unp_label)

print(
    f'Test Loss: {test_loss.result()}, '
    f'Test HR@1: {test_accuracy_1.result() * 100},'
    f'Test HR@5: {test_accuracy_5.result() * 100},'
    f'Test HR@10: {test_accuracy_10.result() * 100},'
    # 'Test NDCG@1: ', ndcg_1,
    'Test NDCG@5: ', ndcg_5,
    'Test NDCG@10: ', ndcg_10,
)



"""case"""

from sklearn.metrics import ndcg_score

def test_step(data_test, sre_test, sre_labels, rel_test, rel_labels, unp_test, unp_labels):
  # training=False is only needed if there are layers with different
  # behavior during training versus inference (e.g. Dropout).
  predictions, pair_sre, pair_rel, pair_unp = model(data_test, sre_test, rel_test, unp_test, training=True)
  t_loss = loss_object(sre_labels, predictions)

  test_loss(t_loss)
  test_accuracy_1(sre_labels, predictions)
  test_accuracy_5(sre_labels, predictions)
  test_accuracy_10(sre_labels, predictions)

  ndcg_1 = ndcg_score(sre_labels, predictions, k=1)
  ndcg_5 = ndcg_score(sre_labels, predictions, k=5)
  ndcg_10 = ndcg_score(sre_labels, predictions, k=10)

  return ndcg_1, ndcg_5, ndcg_10, predictions

pred_store = []
for test_data, sre_test, sre_label, rel_test, rel_label, unp_test, unp_label in test_ds:
    ndcg_1, ndcg_5, ndcg_10, pred = test_step(test_data, sre_test, sre_label, rel_test, rel_label, unp_test, unp_label)
    pred_store = pred_store + pred.numpy().tolist()

print(
    f'Test Loss: {test_loss.result()}, '
    f'Test HR@1: {test_accuracy_1.result() * 100},'
    f'Test HR@5: {test_accuracy_5.result() * 100},'
    f'Test HR@10: {test_accuracy_10.result() * 100},'
    # 'Test NDCG@1: ', ndcg_1,
    'Test NDCG@5: ', ndcg_5,
    'Test NDCG@10: ', ndcg_10,
)

pred_store[0]

import os
user_path = 'D:/WWW/data/user_pre_test'
user_list = os.listdir(user_path)[-469:]

import pandas as pd
i_embed = pd.read_csv('D:/WWW/data/items_encoded_45.csv',header=None)

item_list = pd.read_csv('D:/WWW/data/item_list.csv').set_index('index')

from sklearn.metrics.pairwise import cosine_similarity
for i in range(34,len(user_list)):
    user = user_list[i]
    item_top_id = []
    print('user id: ',user, i)
    user_items = pd.read_csv(os.path.join(user_path,user))

    u_path = 'D:/WWW/data/user_pre'
    path = os.path.join(u_path, user)
    user_df = pd.read_csv(path)
    u_checklist = user_df['item'].values
    x = 0

    user_pred = pred_store[i]
    idx = np.argpartition(user_pred, -5)[-20:]
    for j in range(len(idx)):
        for k in range(len(i_embed)):
            cos_sim = cosine_similarity(user_items.loc[idx[j]].values.reshape([1,-1]), i_embed.loc[k].values.reshape([1,-1]))[0][0]
            if cos_sim > 0.99:
                i_id = item_list.loc[k]['id']
                if i_id in u_checklist:
                    x = 1
                item_top_id.append(i_id)
                print(i_id, x)
                break
    break

u_checklist.values

idx

u_path = 'D:/WWW/data/user_pre'
path = os.path.join(u_path, 'a7pcbzv90g5u7.csv')
user_df = pd.read_csv(path)

user_df