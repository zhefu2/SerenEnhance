# -*- coding: utf-8 -*-
"""Sigir-2023-unexpectedness.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-uwHshe8oRe4r78rtDxcoL1srX91Rd5O
"""

import os
import pandas as pd
import numpy as np
import json

#data_df = pd.read_csv("../data/SerenLens_Books.csv")  # load SerenLens data
data_df = pd.read_csv("../data/SerenLens.csv")

fp = open("../data/reviews_Books_5.json", "r") # load raw data for Amazon book review
print ("ile name: ", fp.name)
item_id = []
for line in fp:
    item_id.append(json.loads(line)['asin'].lower())
fp.close()

fp = open("../data/meta_Books.json", "r")  # load meta data of Amazon book review
print ("file name: ", fp.name)
meta_item_id = []  # item id
co_id = []  # also buy
co2_id = []  # also view
for line in fp:
    meta_item_id.append(json.loads(line)['asin'].lower())
    co_id.append(json.loads(line)['also_buy'])
    co2_id.append(json.loads(line)['also_view'])
fp.close()
dic_table = pd.DataFrame(meta_item_id)
dic_table['also_buy'] = co_id
dic_table['also_view'] = co2_id
dic_table = dic_table.set_index(0)

# extract unique user id and corresponding item sequence
item_seq = []
user_list = np.unique(data_df['user_id'].values)
for i in range(len(user_list)):
  temp_item_list = data_df[data_df['user_id']==user_list[i]]['item_id'].tolist()
  item_seq.append(temp_item_list)
user_table = pd.DataFrame(user_list,columns=['user'])
user_table['item seq']= item_seq

# match co-occurred items for each user
user_co_item = []
for i in range(len(user_table)):
  temp_co = []
  for j in range(len(item_seq)):
    item_current_seq = item_seq[j]
    # merge also buy and also view
    if item_current_seq in dic_table.index.values:
      if type(dic_table.loc[item_current_seq]['also_buy'])!=list:
          temp_co = temp_co + dic_table.loc[item_current_seq]['also_buy'][0] + dic_table.loc[item_current_seq]['also_view'][0]
      else:
          temp_co = temp_co + dic_table.loc[item_current_seq]['also_buy'] + dic_table.loc[item_current_seq]['also_view']
  user_co_item.append(temp_co)
user_table['co-occurrence item'] = user_co_item
# save the data
user_table.to_csv('../data/co_occurrence condition.csv')

# calculate p(i) - item popularity
item_list = np.unique(item_id)  # item list
item_pop = np.zeros((len(item_list)))  # item count matrix
item_ix = np.arange(0,len(item_list))  # item index in the matrix
item_df = pd.DataFrame(item_list, columns=['item_id'])
item_df['item index'] = item_ix
item_df = item_df.set_index('item_id')

for i in range(len(item_id)):  # count occurrence of each item
    ix = item_df.loc[item_id[i]]['item index']
    item_pop[ix] = item_pop[ix] + 1
item_df['count'] = item_pop
item_pob = []
for i in range(len(item_pop)): # calculate the probability
    item_pob.append(item_pop[i]/len(item_list))
item_df['probability'] = item_pob
item_df.to_csv('item_popularity.csv')

# calculate p(i|u) - probability of an item being selected by user based on history
item_dic = np.unique(dic_table.index)
item_serelens_list = np.unique(data_df['item_id'].values) # items in SerenLens

for i in range(len(item_serelens_list)):
    ix = item_serelens_list[i]
    item_co_count = np.zeros((len(item_list)))
    item_df_co = pd.DataFrame(item_list,columns=['item_id'])
    if ix in item_dic:  # extract co-occurred items of the target item ix
        if type(dic_table.loc[ix]['also_buy'])!=list:
            also_b = dic_table.loc[ix]['also_buy'][0]
            also_v = dic_table.loc[ix]['also_view'][0]
        else:
            also_b = dic_table.loc[ix]['also_buy']  
            also_v = dic_table.loc[ix]['also_view']

        # calculate n(i, ix) for the target item ix  (item_co_count)
        for j in range(len(also_b)):  
            if also_b[j] in item_list:
                p = item_df.loc[also_b[j]]['item index']
                p = int(p)
                item_co_count[p] = item_co_count[p] + 1
        for k in range(len(also_v)):
            if also_v[k] in item_list:
                p = item_df.loc[also_v[k]]['item index']
                p = int(p)
                item_co_count[p] = item_co_count[p] + 1
    item_df_co['count'] = item_co_count
    item_df_co['popularity'] = item_df['probability'].values
    file_name = '../data/co_occurrence/' + ix + '.csv'
    item_df_co.to_csv(file_name)
    
# smoothing
item_co_list = os.listdir('../data/co_occurrence/')
file_dir = '../data/co_occurrence/'
miu = 1  # smooth factor
for i in range(len(item_co_list)):
    file_path = file_dir + item_co_list[i]
    item_co_temp = pd.read_csv(file_path)
    n_total = 0
    new_count_list = []
    for j in range(len(item_co_temp)):
        pi = item_co_temp.loc[j]['popularity']
        pc = item_co_temp.loc[j]['count']
        new_count = pc + miu * pi  # n(i, ix) + miu* popularity
        n_total = n_total + pc # sum(n(i, ix))
        new_count_list.append(new_count)
    new_prob = np.array(new_count_list)/(n_total+ miu)  # smoothing p(i|u)
    item_co_temp['smooth_probability'] = new_prob

    file_name = '../data/co_occurrence_smooth/' + item_co_list[i]
    item_df_co.to_csv(file_name)

data_df.head()